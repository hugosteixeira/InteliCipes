{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtrl3pk4XI-f",
        "colab_type": "text"
      },
      "source": [
        "Importamos os o dataset e Convertemos os dados do 'Ingredientes'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dNsGA-pWzqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "with np.load('simplified-recipes-1M.npz',allow_pickle=True) as data:\n",
        "    recipes = data['recipes']\n",
        "    ingredients = data['ingredients']\n",
        "def getRecipes(Recipes):\n",
        "\n",
        "    temp = \"\"\n",
        "    print(Recipes)\n",
        "    for n in Recipes:\n",
        "        temp += \"{} \".format(n.replace(\" \",\"_\"))\n",
        "    temp += \"\\n\"\n",
        "    return temp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y3OKGyvX9Hp",
        "colab_type": "text"
      },
      "source": [
        "Salvamos um arquivo que vai ser usado no treinamento de um Word2Vec usando Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un1lqUa5YgLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\n",
        "    \"dataset.txt\",\n",
        "    mode=\"w\"\n",
        "\n",
        ") as dataset:\n",
        "    for y in range(len(recipes)):\n",
        "        dataset.writelines(getRecipes(ingredients[recipes[y]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7HhLXmfZBeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.models\n",
        "class Training(object):\n",
        "     def __iter__(self):\n",
        "         file = open(\"dataset.txt\")\n",
        "         for line in file:\n",
        "            yield utils.simple_preprocess(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvfZgszcZYKH",
        "colab_type": "text"
      },
      "source": [
        "Um modelo Word2vec é treinado e salvo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArhvYfXWbB83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = Training()\n",
        "model = gensim.models.Word2Vec(sentences=sentences,workers=4)\n",
        "model.wv.save(\"file\")\n",
        "model = gensim.models.Word2Vec.load(\"file\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V13ji9U_bs_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Exemplo de saida:\n",
        "salt -1.6916947 -0.96942025 -0.9606158 2.193035 -1.7667061 -0.8520473 0.25898525 -0.119274154 2.0012894 0.73267627 -0.7832749 1.8030225 -1.5116467 -0.99332505 -1.619654 1.1241528 0.44378647 -0.81558657 1.9063818 -0.6207571 -0.888155 0.8983902 2.8275895 0.4936805 1.1689482 3.0300963 0.630989 0.018772107 -3.8044817 3.055093 1.0198493 0.9456562 -0.54736435 2.209564 -2.047132 0.16281295 0.21697174 -1.5426836 -2.4335616 3.8788924 -1.7889303 1.6647011 -2.0860698 1.9850205 1.7211894 1.6137186 1.045554 2.7409713 0.104628906 2.3747032 -2.7388685 -2.2151659 2.225852 -1.3266102 -0.23949507 0.45789406 -0.5925729 -0.5265513 -1.9072063 0.47278142 2.2958724 -2.2539687 -0.46489477 0.6398083 0.86335534 1.7815746 -0.60370725 2.5461977 2.7497373 -1.5743477 -0.10349743 -1.3141254 -1.1403865 0.1443511 0.43428892 1.6033344 -1.5691049 0.8069743 0.3045501 -2.3461323 0.95178074 1.1830695 0.69550824 0.20946342 1.0494226 -1.2350723 -0.1896006 -3.5973315 -1.5402473 -0.7204545 -1.5966697 0.8332947 -0.75739217 -1.6327658 0.1787824 -0.7793719 2.4284537 3.9902573 -2.309061 0.4402036\n",
        "pepper -1.3083518 -3.5293972 -0.18811043 0.1805113 -0.58460516 -2.7054582 1.6600615 1.327884 -1.4385188 -1.2584811 -3.5587046 1.4335842 -0.5980249 -2.5650094 -1.8087434 -0.9632743 -0.27853093 -0.33647686 -1.849356 -1.3353374 -0.043777 1.6883824 2.0639534 -0.8830288 -2.082216 0.6186435 -1.3404293 1.6730056 -4.1958504 -1.1550456 -0.47636777 0.7602971 -1.0608125 2.8419273 -1.5074803 -0.1335606 0.4279068 0.37743387 0.25025696 2.1431181 -0.6380461 1.254138 -1.0599992 -1.3577172 0.37362418 -0.91297793 -1.9818121 -0.5249386 1.218993 0.4022278 1.4028645 -0.7309707 1.8277367 2.3433564 1.6674274 2.14233 -4.534781 1.9059987 -2.8560925 2.5977986 1.1016879 1.5987278 0.88263947 0.055537667 -1.0776263 -0.5525391 3.2180746 0.026583113 1.5390475 2.2966871 0.9738105 4.5979185 0.7871243 -0.12014082 2.7195973 3.5377195 -0.14246769 4.9528527 -0.41411477 0.095956706 -1.2295349 2.2094624 2.0801487 -1.085023 -1.6100949 1.5546231 -1.3760626 -4.734265 -1.7400037 -2.5592823 -3.4193215 -1.9251108 -2.2522662 1.6676239 0.90412635 -1.7965208 0.17124642 1.9371287 -0.14145787 -0.041629907\n",
        "sugar 1.2373984 -2.2992718 0.31887162 3.2162235 0.4612999 -2.25094 -0.55690527 0.45533597 0.42914933 1.0964341 -3.4675593 0.73658335 3.6146924 -2.6124694 -1.399785 -1.0404118 -1.0712559 -1.8343548 0.5518481 0.5179309 -2.989244 -0.24463557 -1.2438664 2.539401 2.7476854 0.6762741 -1.0024421 1.6727746 -2.8351438 1.3635807 -0.015544447 0.8259849 -2.2367313 0.0073990724 0.5080055 1.798984 1.0160028 1.0168536 -3.026947 1.8565748 0.288281 1.7156589 0.4446134 0.12139526 4.5634236 3.916443 -1.7773911 -3.055621 0.40502545 1.359377 -0.49889395 -1.0395081 -0.13963753 -0.64502317 2.3296633 -1.915802 1.3536434 0.8304885 -0.1732839 -1.0681093 0.41714582 -3.5067596 -0.08869108 -2.0546932 0.062386323 -0.93975455 -0.10888591 -0.81916285 -0.13754715 -3.0242476 3.7491152 -0.5997407 0.9625587 -3.2891214 -0.98773247 -0.29541975 -0.27540717 2.95889 -1.3509068 -0.2657442 0.10004657 3.3453217 -0.5011961 0.7493647 -0.3023838 -1.8828723 -2.2044504 0.94808394 -0.8166061 -2.2989078 -0.84485656 0.10568352 -1.3289701 -1.4274588 -0.23213881 -1.4220972 2.6751819 2.03984 -1.0978981 0.004247455\n",
        "garlic -2.544893 0.84377444 -0.53909373 -0.8767051 3.321837 1.477064 0.2562592 0.35678533 0.5420981 -1.6614168 -0.3372714 -5.14216 1.2003645 0.95849043 -0.87037534 -2.8579254 1.8589282 1.4266158 -3.519044 -1.2408127 -3.1507595 -0.46470562 5.5748277 -3.0303771 -2.3215148 -0.8808247 3.5141575 1.6807034 -0.25998208 -2.0588722 -1.9128903 -0.5646833 2.3937116 -0.5037071 -0.022108667 1.5790032 -0.25229302 -1.6658759 0.41167334 2.1935952 -4.0582128 -5.0487823 0.5109253 3.8416383 -0.64024985 -3.0706813 1.078574 -0.3469913 -2.5643656 0.32455972 -0.21759722 1.4897232 -0.4148891 2.480896 0.1431655 -0.2798186 -2.4021685 -3.6715138 -3.2627776 -0.41217396 -0.23166096 1.3394727 1.3248262 3.1941836 0.067412555 -3.1780663 1.8344343 -0.6565721 -3.304843 1.202521 -0.26735735 4.2578325 -0.38461328 1.8889887 6.093472 2.054265 2.7247813 0.31365088 6.1683297 -3.2377586 -2.1741736 1.5129653 2.4007757 2.642784 -1.7041894 -3.0983205 0.22649823 -1.2952741 0.88084227 -2.0789347 -0.12132811 1.7403283 0.5817161 -1.897759 -1.2424265 0.09018215 2.5286326 -4.3023934 1.6494292 1.4067767\n",
        "butter -0.64952105 2.049714 0.01884324 -1.6064754 1.1153479 -0.56116337 -3.1309073 -0.22167248 3.2055323 -1.5141155 3.8303623 -2.098683 3.0872805 -0.14378077 2.0272007 0.6446417 -0.844683 -0.70944935 -0.4096228 -0.4012664 0.2387875 0.038728304 -0.45368022 -1.5571839 -1.1049279 -0.31477407 -1.9989916 0.060311805 2.9289327 0.9879991 -1.7369218 -0.50491905 1.5842484 -0.3781934 -1.8395803 1.9550734 -0.102319375 3.694445 0.09278746 1.2582293 0.38892484 0.59566224 0.06814885 -0.907834 -1.3636587 0.7055688 1.9194812 -1.5735232 -0.8544731 0.8835319 1.2026507 1.0035731 -0.87367344 -0.31925178 -0.7967642 -0.73956716 2.2432773 -2.6387272 1.3974141 -2.8856616 -1.2353585 0.6578438 -0.46146706 -0.03963189 0.6336838 -1.3356582 -1.1226399 1.5634649 -0.8563888 -2.190735 -1.042578 -2.0554345 -1.5770937 0.10855405 0.27438486 -1.5199071 -0.9396366 1.2587224 0.031768482 4.0598307 -1.5616914 -2.3961022 1.4229487 0.37357122 -2.9923308 0.2941999 0.20213108 -0.8863587 1.0713617 2.5221095 2.5524383 0.59180474 0.87970495 -3.608443 0.9418569 -1.2928636 1.3025496 -4.361121 0.7492046 2.7148392\n",
        "flour -4.1063175 2.7713609 -1.6044366 0.59908336 1.8057995 1.2811108 1.7984158 -1.1115069 0.56574863 0.47860435 -0.29522255 -0.46793526 3.2424164 -2.7973764 2.6719565 0.15507582 3.459352 -1.6754564 -1.6520886 -4.2231793 -2.8859932 -0.49628586 0.8005904 -0.029993838 1.103685 0.04618519 1.0183506 0.5441274 2.5894234 0.038014967 0.69138736 1.2063915 -0.21319863 -0.2887251 -1.596773 1.303025 0.45198718 3.542599 0.393006 0.29191703 -0.15822941 0.82114005 0.7872252 -2.161442 -1.5390695 -2.11562 0.45058024 -1.3624192 -4.3965435 3.9011936 -0.061994545 1.7291598 -0.15180571 -0.53380334 1.4022803 2.139406 -1.9222815 -2.8965478 1.3173304 -1.0577655 -0.07165266 -1.4913287 -1.1252321 -1.7445431 -0.14102866 -4.3419333 -0.04655347 -1.750053 3.1666746 -0.12258977 1.5434793 0.029532174 -0.97175986 -0.9427422 -1.6447848 2.097629 -2.6611629 1.4718558 2.5834577 -0.06429131 -0.31536248 0.6637711 -2.1595507 2.0241394 -4.045377 1.2135777 1.4684895 0.7304763 4.1692004 1.1393209 -1.3213215 2.504156 2.4668984 -2.4312184 1.4034822 -1.0976368 1.1371028 -1.4630663 0.080325186 -0.5171691\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXW_9E3Sbjk6",
        "colab_type": "text"
      },
      "source": [
        "Para a montagem do grafico utilizamos 2 metodos de reduçao: IncrementalPCA e t-SNE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F7EGYx2cKf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE                   # final reduction\n",
        "import numpy as np                                  # array handling\n",
        "from IPython import get_ipython\n",
        "from sklearn.decomposition import \n",
        "\n",
        "def reduce_dimensions(model):\n",
        "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
        "\n",
        "    vectors = [] # positions in vector space\n",
        "    labels = [] # keep track of words to label our data again later\n",
        "    for word in model.wv.vocab:\n",
        "        vectors.append(model.wv[word])\n",
        "        labels.append(word)\n",
        "\n",
        "    # convert both lists into numpy vectors for reduction\n",
        "    vectors = np.asarray(vectors)\n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    # reduce using IncrementalPCA\n",
        "    vectors = np.asarray(vectors)\n",
        "    tsne = IncrementalPCA(n_components=num_dimensions)    \n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    # reduce using t-SNE\n",
        "    vectors = np.asarray(vectors)\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=)     #TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    x_vals = [v[0] for v in vectors]\n",
        "    y_vals = [v[1] for v in vectors]\n",
        "    return x_vals, y_vals, labels\n",
        "\n",
        "\n",
        "x_vals, y_vals, labels = reduce_dimensions(model)\n",
        "\n",
        "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=False):\n",
        "    from plotly.offline import init_notebook_mode, iplot, plot\n",
        "    import plotly.graph_objs as go\n",
        "\n",
        "    trace = go.Scatter(x=x_vals, y=y_vals, mode='markers', text=labels, marker=dict(\n",
        "        size=16,\n",
        "        color=[x_vals[x]*y_vals[x] for x in range(len(x_vals))], #set color equal to a variable\n",
        "        colorscale='Viridis', # one of plotly colorscales\n",
        "        showscale=True))\n",
        "    data = [trace]\n",
        "\n",
        "    if plot_in_notebook:\n",
        "        init_notebook_mode(connected=True)\n",
        "        iplot(data, filename='word-embedding-plot')\n",
        "    else:\n",
        "        plot(data, filename='word-embedding-plot.html')\n",
        "\n",
        "plot_with_plotly(x_vals, y_vals, labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}